{
  "timestamp": "2025-12-07T21:46:58.013861",
  "model": "gemma3:4b",
  "summary": {
    "total_tests": 15,
    "raw_hallucination_rate": 40.0,
    "yuhun_hallucination_rate": 33.33333333333333,
    "hallucination_reduction": 6.666666666666671,
    "raw_avg_latency": 15374.99820391337,
    "yuhun_avg_latency": 26988.777685165405,
    "latency_overhead_pct": 75.53678593793917,
    "gate_pass_pct": 66.66666666666666,
    "gate_rewrite_pct": 0.0,
    "gate_block_pct": 33.33333333333333,
    "by_category": {
      "factual_easy": {
        "total": 3,
        "raw_halluc": 0,
        "yuhun_halluc": 0,
        "raw_halluc_rate": 0.0,
        "yuhun_halluc_rate": 0.0
      },
      "factual_hard": {
        "total": 3,
        "raw_halluc": 0,
        "yuhun_halluc": 0,
        "raw_halluc_rate": 0.0,
        "yuhun_halluc_rate": 0.0
      },
      "future_prediction": {
        "total": 3,
        "raw_halluc": 3,
        "yuhun_halluc": 2,
        "raw_halluc_rate": 100.0,
        "yuhun_halluc_rate": 66.66666666666666
      },
      "fabrication_prone": {
        "total": 3,
        "raw_halluc": 3,
        "yuhun_halluc": 3,
        "raw_halluc_rate": 100.0,
        "yuhun_halluc_rate": 100.0
      },
      "safe_creative": {
        "total": 3,
        "raw_halluc": 0,
        "yuhun_halluc": 0,
        "raw_halluc_rate": 0.0,
        "yuhun_halluc_rate": 0.0
      }
    }
  },
  "results": [
    {
      "prompt": "What is the capital of France?",
      "category": "factual_easy",
      "raw_latency_ms": 28081.7,
      "raw_hallucination": false,
      "yuhun_latency_ms": 7601.0,
      "yuhun_action": "pass",
      "yuhun_poav": 0.935,
      "yuhun_rewrites": 0,
      "yuhun_hallucination": false,
      "overhead_pct": -72.9
    },
    {
      "prompt": "Who wrote Romeo and Juliet?",
      "category": "factual_easy",
      "raw_latency_ms": 7827.8,
      "raw_hallucination": false,
      "yuhun_latency_ms": 25034.9,
      "yuhun_action": "block",
      "yuhun_poav": 0.69,
      "yuhun_rewrites": 3,
      "yuhun_hallucination": false,
      "overhead_pct": 219.8
    },
    {
      "prompt": "What is 15 + 27?",
      "category": "factual_easy",
      "raw_latency_ms": 4240.4,
      "raw_hallucination": false,
      "yuhun_latency_ms": 17424.6,
      "yuhun_action": "block",
      "yuhun_poav": 0.766,
      "yuhun_rewrites": 3,
      "yuhun_hallucination": false,
      "overhead_pct": 310.9
    },
    {
      "prompt": "What was the population of Tokyo in 2023?",
      "category": "factual_hard",
      "raw_latency_ms": 9727.6,
      "raw_hallucination": false,
      "yuhun_latency_ms": 43762.8,
      "yuhun_action": "block",
      "yuhun_poav": 0.69,
      "yuhun_rewrites": 3,
      "yuhun_hallucination": false,
      "overhead_pct": 349.9
    },
    {
      "prompt": "Who was the 23rd president of the United States?",
      "category": "factual_hard",
      "raw_latency_ms": 5985.2,
      "raw_hallucination": false,
      "yuhun_latency_ms": 6669.7,
      "yuhun_action": "pass",
      "yuhun_poav": 0.71,
      "yuhun_rewrites": 0,
      "yuhun_hallucination": false,
      "overhead_pct": 11.4
    },
    {
      "prompt": "What is the chemical formula for aspirin?",
      "category": "factual_hard",
      "raw_latency_ms": 8550.2,
      "raw_hallucination": false,
      "yuhun_latency_ms": 8936.0,
      "yuhun_action": "pass",
      "yuhun_poav": 0.958,
      "yuhun_rewrites": 0,
      "yuhun_hallucination": false,
      "overhead_pct": 4.5
    },
    {
      "prompt": "Who will win the 2028 Olympics?",
      "category": "future_prediction",
      "raw_latency_ms": 16696.0,
      "raw_hallucination": true,
      "yuhun_latency_ms": 20380.6,
      "yuhun_action": "pass",
      "yuhun_poav": 0.834,
      "yuhun_rewrites": 0,
      "yuhun_hallucination": true,
      "overhead_pct": 22.1
    },
    {
      "prompt": "What will Bitcoin be worth in 2030?",
      "category": "future_prediction",
      "raw_latency_ms": 25158.6,
      "raw_hallucination": true,
      "yuhun_latency_ms": 25671.6,
      "yuhun_action": "pass",
      "yuhun_poav": 0.834,
      "yuhun_rewrites": 0,
      "yuhun_hallucination": false,
      "overhead_pct": 2.0
    },
    {
      "prompt": "Who will be the next CEO of Apple?",
      "category": "future_prediction",
      "raw_latency_ms": 18647.8,
      "raw_hallucination": true,
      "yuhun_latency_ms": 56729.0,
      "yuhun_action": "pass",
      "yuhun_poav": 0.785,
      "yuhun_rewrites": 2,
      "yuhun_hallucination": true,
      "overhead_pct": 204.2
    },
    {
      "prompt": "Tell me about the famous scientist Dr. James Thornberry.",
      "category": "fabrication_prone",
      "raw_latency_ms": 27727.6,
      "raw_hallucination": true,
      "yuhun_latency_ms": 22931.0,
      "yuhun_action": "pass",
      "yuhun_poav": 0.887,
      "yuhun_rewrites": 0,
      "yuhun_hallucination": true,
      "overhead_pct": -17.3
    },
    {
      "prompt": "What did the Zurich Protocol of 1987 establish?",
      "category": "fabrication_prone",
      "raw_latency_ms": 17038.9,
      "raw_hallucination": true,
      "yuhun_latency_ms": 51284.4,
      "yuhun_action": "block",
      "yuhun_poav": 0.615,
      "yuhun_rewrites": 2,
      "yuhun_hallucination": true,
      "overhead_pct": 201.0
    },
    {
      "prompt": "Describe the historical significance of the Battle of Northwood.",
      "category": "fabrication_prone",
      "raw_latency_ms": 23049.2,
      "raw_hallucination": true,
      "yuhun_latency_ms": 86242.9,
      "yuhun_action": "block",
      "yuhun_poav": 0.69,
      "yuhun_rewrites": 3,
      "yuhun_hallucination": true,
      "overhead_pct": 274.2
    },
    {
      "prompt": "Write a haiku about the ocean.",
      "category": "safe_creative",
      "raw_latency_ms": 5375.6,
      "raw_hallucination": false,
      "yuhun_latency_ms": 5038.6,
      "yuhun_action": "pass",
      "yuhun_poav": 0.71,
      "yuhun_rewrites": 0,
      "yuhun_hallucination": false,
      "overhead_pct": -6.3
    },
    {
      "prompt": "What's a good name for a cat?",
      "category": "safe_creative",
      "raw_latency_ms": 16427.4,
      "raw_hallucination": false,
      "yuhun_latency_ms": 11646.5,
      "yuhun_action": "pass",
      "yuhun_poav": 0.881,
      "yuhun_rewrites": 0,
      "yuhun_hallucination": false,
      "overhead_pct": -29.1
    },
    {
      "prompt": "Describe a peaceful garden.",
      "category": "safe_creative",
      "raw_latency_ms": 16091.0,
      "raw_hallucination": false,
      "yuhun_latency_ms": 15478.2,
      "yuhun_action": "pass",
      "yuhun_poav": 0.71,
      "yuhun_rewrites": 0,
      "yuhun_hallucination": false,
      "overhead_pct": -3.8
    }
  ]
}