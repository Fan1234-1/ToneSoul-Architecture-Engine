from datetime import datetime, timezone
import uuid
import sys
import os

# Ensure we can import yuhun package (proper path handling)
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

from yuhun.state_store import load_state, save_state, load_island, save_island, append_chronicle
from yuhun.ollama_client import generate
from yuhun.gate import build_plan
from yuhun.prompt_builder import build_prompt
from yuhun.meta_parser import extract_meta
from yuhun.models import ChronicleEntry, FSVector

# Input limits
MAX_INPUT_LEN = 2000
MAX_SUMMARY_LEN = 200

def main():
    print("YuHun-Orchestrator v0.1 Ã— Ollama")
    state = load_state()
    island = load_island(state.active_island)
    if island is None:
        # Fallback if state points to non-existent island (shouldn't happen with load_state logic but good for safety)
        print(f"Warning: Active island {state.active_island} not found. Creating new one.")
        island_id = str(uuid.uuid4())[:8]
        from yuhun.models import TimeIsland
        island = TimeIsland(
            island_id=island_id,
            created_at=datetime.now(timezone.utc).isoformat(),
            title="Recovered Island",
        )
        state.active_island = island_id
        save_island(island)
        save_state(state)

    while True:
        try:
            user_input = input("\nä½ ï¼š").strip()[:MAX_INPUT_LEN]  # Apply input limit
        except EOFError:
            break
            
        if user_input.lower() in ("exit", "quit", "bye"):
            print("ğŸ‘‹ çµæŸå°è©±ã€‚")
            break
        
        if not user_input:
            continue

        # 1. Gate
        plan = build_plan(state, user_input)

        # 2. çµ„ prompt
        prompt = build_prompt(state, island, plan, user_input)

        # 3. å‘¼å« Ollama (with error handling)
        print(f"Thinking... (Mode: {plan['mode']}, Model: {plan['model']})")
        try:
            raw_response = generate(plan["model"], prompt)
        except Exception as e:
            print(f"âš ï¸ LLM call failed: {e}")
            raw_response = f"æŠ±æ­‰ï¼Œæˆ‘æš«æ™‚ç„¡æ³•è™•ç†é€™å€‹è«‹æ±‚ã€‚éŒ¯èª¤ï¼š{type(e).__name__}"

        # 4. æ‹†å‡º meta
        clean_response, meta = extract_meta(raw_response)

        # 5. æ›´æ–° FS
        fs_before = FSVector(**state.fs.__dict__)
        state.fs.C += meta.fs_delta.get("C", 0)
        state.fs.M += meta.fs_delta.get("M", 0)
        state.fs.R += meta.fs_delta.get("R", 0)
        state.fs.Gamma += meta.fs_delta.get("Gamma", 0)
        # clamp åˆ° [0,1]
        for k in ["C", "M", "R", "Gamma"]:
            v = getattr(state.fs, k)
            setattr(state.fs, k, max(0.0, min(1.0, v)))

        # 6. ç°¡å–® semantic_tension æ›´æ–°
        island.semantic_tension = plan["delta_s"]
        island.current_mode = meta.mode_used
        island.last_step_id = step_id = str(uuid.uuid4())[:8]

        # 7. å¯« Chronicle
        entry = ChronicleEntry(
            step_id=step_id,
            island_id=island.island_id,
            timestamp=datetime.now(timezone.utc).isoformat(),
            user_input=user_input[:MAX_INPUT_LEN],  # Apply limit
            model_reply_summary=clean_response[:MAX_SUMMARY_LEN],  # Use constant
            mode_used=meta.mode_used,
            fs_before=fs_before,
            fs_after=state.fs,
            tools_used=[],  # v0 å…ˆä¸å¯¦ä½œå·¥å…·
            notes="",
        )
        append_chronicle(entry)

        # 8. å­˜ state & island
        save_state(state)
        save_island(island)

        # 9. å›çµ¦ä½ 
        print(f"\nYuHun[{meta.mode_used}]ï¼š{clean_response}")
        print(f"(FS: C={state.fs.C:.2f}, M={state.fs.M:.2f}, R={state.fs.R:.2f}, Î“={state.fs.Gamma:.2f}, Î”S={island.semantic_tension:.2f})")

if __name__ == "__main__":
    main()
